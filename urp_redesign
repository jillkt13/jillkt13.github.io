# Contextualizing new content types

Due to strategic business changes, JSTOR, traditionally a host of academic content like journal articles, was moving to a more diverse multi-content model, including video, audio, and images. Furthermore, this content would no longer come from academic publishers, but individual contributors with potentially laxer standards regarding content curation. 

To meet one of our product OKRs, I conducted exploratory research to understand how our users might contextualize these changes. My goal was to identify complementary information that would be helpful to users when encountering these new types of content. 

**My challenge:** Identify complementary information that would signal the differences in new content to the JSTOR platform, while also maintaining the value of our site’s curation over decades and enriching our overall content offering for users

**My Contribution:** project scope, survey design, study design, protocol development, moderation, data analysis, presentation


## Method and Approach

Put high level graphic of the three stages here. 

### Design Jam

<img src="/images/jam_small_photo.png" width="300">

By analyzing prior research, we identified the categories of authority, quality, curation, provenance, and access as being potentially significant information to signal to users in a multi-content environment. The design and research teams then sketched ideas of how we might communicate these five categories on the content page. After the jam, I reviewed all the ideas and developed a list of possible contextual information to signal to users. 

### Survey

Coming out of the design jam, I had a list of over 30 pieces of information that we could communicate to users. We had gone from zero ideas to too many ideas. Therefore, I designed a survey asking users to rank the information most important to them when evaluating content on our website. 

I analyzed over 300 responses to sort the information into three categories: Highest importance, Medium importance, and Lowest importance. After taking into account business needs as well as our technical capacity, I decided upon seven items of contextual information to communicate on our content page and worked with two designers on possible UI elements for signaling the information.

High importance | Medium importance | Low importance
------------ | ------------- | -------------
Type of content | Rights of use | Number of views
Date of creation | Part of a broader collection | Current location
Similar items on JSTOR | Quality of item | Who acquired
Where it's originally from | Rights of consumption | Current curator of item

### User interviews and mapping activity (Remote and in-person)

At this point, I started to think through how to test these designs with users. What I really wanted to know was whether the inputs matched the outputs. That is, we had conducted the design jam and developed a list of information to signal based on our internally stated goal of communicating the authority, curation, access, quality, and provenance of content. If we were successful, users should be able to link information found in the UI to these broader overarching concepts. 

I conducted 11 interviews with students, faculty, and researchers: 5 in-person and 6 remote. I started with semi-structured questions, asking participants what information was most important to them when evaluating content for academic work. 

Some questions from the interview guide:
> When looking at content on JSTOR, how do you evaluate the content? What information are you looking at? 
<br> <br>
> What is the most meaningful information?
<br> <br>
> How does that information indicate the "authority" of the content to you?
<br> <br>
> Is there any contextual information that's not here that you would like to see? 
<br> <br>

The second portion of the interview was an interactive mapping exercise, where participants were presented a prototype with three different content types—image, primary source, and journal article—and asked to place stickers on UI elements that signaled Authority, Curation, Access, Quality, and Provenance, all the while explaining why. 

<pre><img src="/images/Screen Shot 2020-06-25 at 11.17.30 AM.png" width="250">        <img src="/images/Screen Shot 2020-06-25 at 11.17.40 AM.png" width="250">        <img src="/images/Screen Shot 2020-06-25 at 11.21.50 AM.png" width="250"> </pre>


## Impact

* Internally, our organization felt that these changes to content type would be very jarring for users. But this research showed that users are mostly concerned with relevance, i.e., finding the content most useful to them. Extra information, while occasionally appreciated, was just that—extra. By interrogating this assumption, our organization was able to work on this new strategic initiative with evidence of what users want. 
* I socialized the above learning very widely: Show and Tell, org-wide Lunch and Learn, and ad hoc meetings with Product Managers. As a result, product teams working to redesign follow two principles I developed: 
  1. Content is king! Despite the drastic changes to the types of content on our platform, the content—as opposed to accompanying contextual information—should remain the focus. 
  2. Contextual information should be discoverable on the content page for those who want it, but not distract from the majority of users who are not looking for it

## Reflections

* *Evaluate your assumptions earlier*, rather than later: From the get go, we designed with the goal of communicating the ideas of authority, curation, access, provenance, and quality on the content page. Our research team had decided on these five categories after synthesizing years of prior user feedback. When we finally put these categories in front of users, however, their mental models didn’t correspond to these five categories. Learning that users don’t think of content in this way ended up being one of the most valuable insights from this project, though we probably could have discovered it much earlier if we had spoken to users earlier in the process.
* *Your ideal method isn’t always possible*: I was set on conducting a sort of first click test using Optimal Workshop. But when the tool simply couldn’t fit the exact need, I had to go back to the drawing board. I was frustrated, but the method I eventually decided on probably led to much richer data, answering not only “what” but also “why”. 




