# Contextualizing new content types

JSTOR is a nonprofit academic database that traditionally hosts text-based academic content like journal articles. Due to strategic business changes, JSTOR was moving to a more diverse multi-content model, including video, audio, and images. Furthermore, this content would no longer come from trusted academic publishers, but individual contributors with potentially laxer standards regarding the curation and quality of their content. 

To meet one of our product OKRs, I conducted exploratory research to understand how our users might contextualize these changes. My goal was to assess the risks as well as to identify complementary information that would be helpful to users when encountering these new types of content. 

**My challenge:** Identify complementary information that would signal the differences in new content to the JSTOR platform, while also maintaining the value of the site’s reputation and curation over the past decades

## Method and Approach

### Design Jam

<img src="/images/jam_small_photo.png" width="300">

When assigned to this project, I was given a summary of prior research done by other team members. In this analysis, they had identified the categories of Authority, Quality, Curation, Provenance, and Access of the content as potentially significant information to signal to users. 

The design and research teams then sketched ideas of how to communicate these five abstract categories on the content page. After the jam, I reviewed the ideas and developed a list of possible contextual information to signal to users that would help them understand the Authority, Quality, Curation, Provenance, and Access to the content. 

### Survey

I now had a list of over 30 pieces of information falling into the 5 categories of Authority, Qualilty, Curation, Provenance, and Access that we could communicate to users. We had gone from zero ideas to too many ideas. Therefore, I designed a survey asking users to rank the information most important to them when evaluating content on the site. 

I analyzed over 500 responses to sort the information into three categories: Highest importance, Medium importance, and Lowest importance. After taking into account business needs as well as technical constraints, I decided upon seven items of contextual information to communicate on the content page and worked with two designers on possible UI elements for signaling the information.

High importance | Medium importance | Low importance
------------ | ------------- | -------------
Type of content | Rights of use | Number of views
Date of creation | Part of a broader collection | Current location
Similar items on JSTOR | Quality of item | Who acquired
Where it's originally from | Rights of consumption | Current curator of item

### User interviews and mapping activity (Remote and in-person)

Next I had to decide how to test these designs with users. What I really wanted to know was whether the inputs matched the outputs. That is, we had conducted the design jam and developed a list of information to signal based on our internally stated goal of communicating the authority, curation, access, quality, and provenance of content. If we had been successful, users would be able to link information found in the UI to these broader overarching concepts. 

I conducted 11 interviews with students, faculty, and researchers: 5 in-person and 6 remote. I started with semi-structured questions, asking participants what information was most important to them when evaluating content for academic work. 

Some questions from the interview guide:
> When looking at content on JSTOR, how do you evaluate the content? What information are you looking at? 
<br> <br>
> What is the most meaningful information?
<br> <br>
> How does that information indicate the "authority" of the content to you?
<br> <br>
> Is there any contextual information that's not here that you would like to see? 
<br> <br>

The second portion of the interview was an interactive mapping exercise, where participants were presented three prototypes with three different content types—image, primary source, and journal article—and asked to place stickers on UI elements that signaled Authority, Curation, Access, Quality, and Provenance, all the while explaining why. They were given definitions of the each category as well as a corresponding sticker color. So when something in the UI indicated Authority to them, they'd place a blue sticker. 

<pre><img src="/images/Screen Shot 2020-06-25 at 11.17.30 AM.png" width="250">        <img src="/images/Screen Shot 2020-06-25 at 11.17.40 AM.png" width="250">        <img src="/images/Screen Shot 2020-06-25 at 11.21.50 AM.png" width="250"> </pre>

The artifacts above emerged from the mapping exercise. I analyzed them to develop an actionable list of information and UI elements that best helped users understand and contextualize content. I also created and shared a summary of how users evaluate content more generally in order to inform all future design work on the content pages. 

#### UI Elements most successful in signaling certain categories
<img src="/images/Screen Shot 2019-10-01 at 9.45.24 AM.png">

## Impact

* Internally, our organization felt that these changes in content type and curation would be very jarring for users. But this research showed that users are mostly concerned with relevance, i.e., finding the content most useful to them. Extra information, while occasionally appreciated, was just that—extra. By interrogating this assumption, our organization was able to work on this new strategic initiative with greater confidence that it would meet users' needs. 
* I socialized the above learning very widely, including an organization-wide meeting and ad hoc meetings with several Product Managers. As a result, product teams working to redesign the content page follow two principles I developed: 
  1. Content is king! Despite the drastic changes to the types of content on our platform, the content—as opposed to accompanying contextual information—should remain the focus. 
  2. Contextual information should be discoverable on the content page for those who want it, but not distract from the majority of users who are not looking for it

## Reflections

* *Evaluate your assumptions earlier, rather than later:* From the get go, we designed with the goal of communicating the ideas of authority, curation, access, provenance, and quality on the content page. Our research team had decided on these five categories after synthesizing years of prior user feedback. When we finally put the categories in front of users, however, their mental models didn’t correspond to these five categories. Learning that users don’t think of content in this way ended up being one of the most valuable insights from this project, though I probably could have discovered it much earlier if I had spoken to users earlier in the process.
* *Your ideal method isn’t always possible*: I was set on conducting a sort of first click test using Optimal Workshop. But when the tool simply couldn’t fit the exact need, I had to go back to the drawing board. I was frustrated, but the method I eventually decided on probably led to much richer data, answering not only “what” but also “why”. 
